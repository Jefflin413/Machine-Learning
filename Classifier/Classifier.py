# Three types classification

import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import imageio
import numpy as np

# Set data
# Set a tensor of value 1 and has dimension 100*2 as base data which contains 2D information
n_data = torch.ones(100,2)
# Using torch.normal(means, std) to generate 3 different types of data
x0 = torch.normal(4*n_data,2)
x1 = torch.normal(-2*n_data,2)
x2 = torch.normal(-8*n_data,2)
# Set y data as label
y0 = torch.zeros(100)
y1 = torch.ones(100)
y2 = 2 * torch.ones(100)

# concatenate both data and labels: data data type: float, label data type: long
# torch.cat make number 0-99 be x0 (y0 = 0), 100-199 be x1 (y1 = 1) and 200-299 be x2 (y2 = 2)
x = torch.cat((x0, x1, x2), 0).type(torch.FloatTensor)  # FloatTensor = 32-bit floating
y = torch.cat((y0, y1, y2), ).type(torch.LongTensor)    # LongTensor = 64-bit integer

# Construct classifier model class
class Net(torch.nn.Module):
    def __init__(self, n_feature, n_hidden, n_output):
        super(Net, self).__init__()
        self.hidden = torch.nn.Linear(n_feature, n_hidden)
        self.out = torch.nn.Linear(n_hidden, n_output)

    def forward(self, x):
        x = F.relu(self.hidden(x))
        x = self.out(x)
        return x
    
# Define classifier with 2D input data, 3 types of output data, 10 hidden nodes
# Choose stochastic gradient descent optimizer
# Choose loss function: CrossEntropyLoss() is a loss function which is particularly strong on classification,
# the loss is generated by comparing the probibility, that is the result of outputs go through softmax function
net = Net(n_feature = 2, n_hidden = 10, n_output = 3)
optimizer = torch.optim.SGD(net.parameters(), lr = 0.02)
loss_func = torch.nn.CrossEntropyLoss()

# Continuously plot
plt.ion()
# Figure
f = plt.figure()
# A container that is used to save images in order to produce gif file 
gif = []

for t in range(700):
    # 300*2 --> 300*3
    out = net(x)
    # The input dimension of loss function 3 vs. 1 can still be accept is because pytorch implicitly make the 3D output of the net to be possibility of predicted type
    loss = loss_func(out, y)
    # Clear last gradient
    optimizer.zero_grad()
    # Backpropagation
    loss.backward()
    # Update parameters
    optimizer.step()

    if t%2 == 0:
        # Clear axis
        plt.cla()
        # the first '1' means it will find the maximum in dimension 1 (while dimension 0 is the serial number), index[1] returns the position of maximum, while [0] will return the value of maximum
        prediction = torch.max(out, 1)[1]
        pred_y = prediction.data.numpy()
        target_y = y.data.numpy()

        # plot
        # scatter(X, Y, color, size, linewidth, colormap)
        # The value stored in color will be converted to color through colormap
        plt.scatter(x.data.numpy()[:,0], x.data.numpy()[:,1], c=pred_y, s=100, lw=0, cmap = 'brg')
        accuracy = float((pred_y == target_y).astype(int).sum())/float(target_y.size)
        plt.text(-4, -12, "Accuracy = %2f" %accuracy, fontdict = {'size':20, 'color': 'black'})

        # Used to return the plot as an image rray
        # Draw the canvas, cache the renderer
        # Save every images in the list 'gif'
        f.canvas.draw()
        image = np.frombuffer(f.canvas.tostring_rgb(), dtype='uint8')
        image  = image.reshape(f.canvas.get_width_height()[::-1] + (3,))
        gif.append(image)

        plt.pause(0.03)

imageio.mimsave('./classifier.gif', gif, fps=10)
print('Finished!!')
plt.ioff()
plt.show()




